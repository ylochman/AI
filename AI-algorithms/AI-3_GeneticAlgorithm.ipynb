{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "code_folding": [
     5,
     29,
     35,
     41,
     53,
     59,
     65,
     73,
     79,
     94,
     125,
     147,
     158,
     169
    ]
   },
   "outputs": [],
   "source": [
    "class genetic_algorithm():#, p_cross=0.5, p_mut=0.5, p_inv=0.5, eps=1e-5):\n",
    "    import random\n",
    "    import math\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self, f, low, high, eps=1e-2):\n",
    "        self.f = f\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.eps = eps\n",
    "        self.d = self.high - self.low\n",
    "        k = int(self.d / self.eps + 1)\n",
    "        gen_len = int(math.log(k, 2)) + 1\n",
    "        self.k = 2 ** gen_len + 1\n",
    "        \n",
    "        self._create_sets()\n",
    "   \n",
    "\n",
    "    def _create_sets(self):\n",
    "        \"\"\"\n",
    "        create phenotype, integer represantation of phenotype and genotype\n",
    "        \"\"\"\n",
    "        self.phenotype = np.linspace(self.low, self.high, self.k)\n",
    "        self.phenotype_int = dict(zip(self.phenotype, range(self.k)))\n",
    "        \n",
    "        self.gen_len = int(math.log(self.k, 2)) + 1\n",
    "        self.genotype = np.array([self._int_to_bin(el) for el in self.phenotype_int.values()])\n",
    "      \n",
    "    \n",
    "    def _int_to_bin(self, int_number):\n",
    "        \"\"\"\n",
    "        return a binary represantation of an integer\n",
    "        \"\"\"\n",
    "        return str(bin(int_number)[2:]).zfill(self.gen_len)\n",
    "        \n",
    "    def _bin_to_int(self, bin_number):\n",
    "        \"\"\"\n",
    "        return an integer decimal represantation of binary\n",
    "        \"\"\"\n",
    "        return int(str(bin_number), 2)\n",
    "   \n",
    "    def _float_to_int(self, float_number):\n",
    "        \"\"\"\n",
    "        return integer representation of float (index of phenotype)\n",
    "        \"\"\"\n",
    "        return self.phenotype_int[float_number]\n",
    "    \n",
    "    def _int_to_float(self, int_number):\n",
    "        \"\"\"\n",
    "        return float of as a phenotype in index\n",
    "        \"\"\"\n",
    "        return self.phenotype[min(int_number,self.k-1)]\n",
    "\n",
    "    def _float_to_bin(self, float_number):\n",
    "        \"\"\"\n",
    "        return composition of _float_to_int & _int_to_bin\n",
    "        \"\"\"\n",
    "        return self._int_to_bin(self._float_to_int(float_number))\n",
    "    \n",
    "    def _bin_to_float(self, bin_number):\n",
    "        \"\"\"\n",
    "        return composition of _bin_to_int & _int_to_float\n",
    "        \"\"\"\n",
    "        return self._int_to_float(self._bin_to_int(bin_number))\n",
    "   \n",
    "    def _find_nearest_element(self, element):\n",
    "        \"\"\"\n",
    "        find nearest element in phenotype to the element\n",
    "        :param: element - float\n",
    "        return float element\n",
    "        \"\"\"\n",
    "        return self.phenotype[argmax(-abs(self.phenotype - element))]\n",
    "\n",
    "    def _evaluate_fitness(self):\n",
    "        \"\"\"\n",
    "        evaluate fitness-function of each element from generation\n",
    "        \"\"\"\n",
    "        self.fitness_of_generation = np.array([self.f(el) for el in self.current_generation])\n",
    "\n",
    "    def selection(self):\n",
    "        \"\"\"\n",
    "        select randomly two elements from elements with fitness-function > mean\n",
    "        create current_parents as an array of two floats\n",
    "        \"\"\"\n",
    "        self._evaluate_fitness()\n",
    "        m = self.fitness_of_generation.mean()\n",
    "        #population = np.extract(self._evaluate_fitness() > m, self.current_generation)\n",
    "        indices = np.where(self.fitness_of_generation > m)[0]\n",
    "        random.shuffle(indices)\n",
    "        self.parents = self.current_generation[indices[:2]] \n",
    "        if self.print_out:\n",
    "            print('-> selected parents: ', self.parents)\n",
    "            print('-> selected parents (int): ', [self.phenotype_int[p] for p in self.parents])\n",
    "\n",
    "    def crossover(self, p=0.5):\n",
    "        \"\"\"\n",
    "        implement 1-point crossover\n",
    "        create two childs from two parents\n",
    "        \"\"\"\n",
    "        #n_coord = selected.shape[1]\n",
    "        #new_element = np.zeros(n_coord)\n",
    "        #for i in range(n_coord):\n",
    "        #    new_element[i] = np.random.choice(selected[:,i], p=[p, 1 - p])\n",
    "        #    print(selected[:,i])\n",
    "        parent0_bin = self._float_to_bin(self.parents[0])\n",
    "        parent1_bin = self._float_to_bin(self.parents[1])\n",
    "        if self.print_out:\n",
    "            print('-> selected parents (binary): ', [parent0_bin, parent1_bin])\n",
    "        point = np.random.randint(self.gen_len) + 1\n",
    "        if self.print_out:\n",
    "            print('-> crossover point: ', point)\n",
    "        self.left_child = parent0_bin[:point] + parent1_bin[point:]\n",
    "        self.right_child = parent1_bin[:point] + parent0_bin[point:]\n",
    "        if self.print_out:\n",
    "            print('-> created childs (binary): ', [self.left_child,\n",
    "                                               self.right_child])\n",
    "        self.left_child = self._bin_to_float(self.left_child)\n",
    "        self.right_child = self._bin_to_float(self.right_child)\n",
    "        if self.print_out:\n",
    "            print('-> created childs (int): ', [self.phenotype_int[self.left_child],\n",
    "                                      self.phenotype_int[self.right_child]])\n",
    "        \n",
    "            print('-> created childs: ', [self.left_child, self.right_child])\n",
    "        \n",
    "        \n",
    "    def mutation(self, element, p=0.01):\n",
    "        \"\"\"\n",
    "        inverse each bit of element with probability p\n",
    "        :param: element - float\n",
    "        return mutated element as float\n",
    "        \"\"\"\n",
    "        if self.print_out:\n",
    "            print('-> element: ', element)\n",
    "        element_bin = self._float_to_bin(element)\n",
    "        if self.print_out:\n",
    "            print('-> element (binary): ', element_bin)\n",
    "        mutated_bin = ''.join(list(map(lambda x: str(1 - int(x))\n",
    "                                       if np.random.choice([0,1], p=[p, 1 - p]) == 0\n",
    "                                       else x, element_bin)))\n",
    "        if self.print_out:\n",
    "            print('-> mutated element (binary): ', mutated_bin)\n",
    "        mutated = self._bin_to_float(mutated_bin)\n",
    "        mutated = self._find_nearest_element(mutated)\n",
    "        if self.print_out:\n",
    "            print('-> mutated element: ', mutated)\n",
    "        return mutated\n",
    "    \n",
    "    def mutation2(self, element, p=0.01):\n",
    "        \"\"\"\n",
    "        inverse each bit of element with probability p\n",
    "        :param: element - a binary represantation, of type list i.e. [0,1,0,1,1]\n",
    "        return a list which is a binary represantation of mutated element\n",
    "        \"\"\"\n",
    "        mutated = list(map(lambda x: 1 - x \n",
    "                           if np.random.choice([0,1], p=[p, 1 - p]) == 0\n",
    "                           else x, element))\n",
    "        return mutated\n",
    "\n",
    "    def mutation3(self, element, p=0.01):\n",
    "        \"\"\"\n",
    "        inverse each bit of element with probability p\n",
    "        :param: element - a binary represantation, of type string i.e. '01011'\n",
    "        return a string which is a binary represantation of mutated element\n",
    "        \"\"\"\n",
    "        mutated = ''.join(list(map(lambda x: str(1 - int(x))\n",
    "                                   if np.random.choice([0,1], p=[p, 1 - p]) == 0\n",
    "                                   else x, element)))\n",
    "        return mutated\n",
    "    \n",
    "    def inversion(self, element, p=0.5):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def run_algorithm(self, p_cross=0.5, p_mut=0.01,\n",
    "                            p_inv=0.5, print_out=True):\n",
    "        \n",
    "        self.print_out = print_out\n",
    "    \n",
    "        self.current_generation = self.phenotype\n",
    "        self.new_generation = []\n",
    "        if self.print_out:\n",
    "            print('-> current generation:', self.current_generation)\n",
    "        #while not(stop_condition(fits)):\n",
    "        for i in range(self.k):\n",
    "            self.selection()\n",
    "            self.crossover(p_cross)\n",
    "            self.left_child  = self.mutation(self.left_child,  p_mut)\n",
    "            self.right_child = self.mutation(self.right_child, p_mut)\n",
    "            #self.left_child  = self.inversion(self.left_child,  p_inv)\n",
    "            #self.right_child = self.inversion(self.right_child, p_inv)\n",
    "            self.new_generation.append(self.left_child)\n",
    "            self.new_generation.append(self.right_child)\n",
    "        if self.print_out:\n",
    "            print('-> new generation:', list[set(self.new_generation)])\n",
    "        self.current_generation = self.new_generation\n",
    "        self._evaluate_fitness()\n",
    "        \n",
    "        return self.current_generation[self.fitness_of_generation.argmax()], self.fitness_of_generation.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 =  0.888671875\n",
      "f0 =  3.44981565533\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: 2 * x - x**3 + (1 + x)**.5  + 1\n",
    "low = 0\n",
    "high = 10\n",
    "GA = genetic_algorithm(f, low, high, eps=0.01)\n",
    "GA.phenotype\n",
    "GA.phenotype_int\n",
    "GA.genotype\n",
    "GA.gen_len\n",
    "GA.k\n",
    "x0, f0 = GA.run_algorithm(print_out=False)\n",
    "print('x0 = ', x0)\n",
    "print('f0 = ', f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.3597285   7.00445783]\n",
      "[ 3.34535538  1.04766183]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 4.66029837,  7.40687085],\n",
       "        [ 4.91780452,  7.68397795],\n",
       "        [ 4.3597285 ,  3.34535538],\n",
       "        [ 7.38964352,  0.95294612],\n",
       "        [ 7.00445783,  1.04766183]]),\n",
       " array([ 76.58011661,  83.22831845,  30.19863521,  55.51493765,  50.16002487]),\n",
       " array([[ 4.3597285 ,  3.34535538],\n",
       "        [ 7.00445783,  1.04766183]]),\n",
       " array([ 4.3597285 ,  3.34535538]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pops = init(k, 2, low, high)\n",
    "fits = evaluate(f, pops)\n",
    "sel = select(pops)\n",
    "pops, fits, sel, crossover(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": [
     0,
     12,
     21,
     30
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_prediction(w, X):\n",
    "    \"\"\"\n",
    "    Returns a linear regression model prediction labels for objects in matrix \n",
    "    X using weights w:\n",
    "    y_pred = (X,w)\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        return(np.insert(X, 0, 1).dot(w))\n",
    "    else:\n",
    "        n = X.shape[0]\n",
    "        return np.dot(np.hstack((np.ones((n,1)),X)),w)\n",
    "\n",
    "def mean_squared_error(y, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a mean squared error between real and predicted labels\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mse = np.sum((y - y_pred)**2) / y.size\n",
    "    return mse\n",
    "\n",
    "def mean_absolute_error(y, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a mean absolute error between real and predicted labels\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mae = np.sum(abs(y - y_pred)) / y.size\n",
    "    return mae\n",
    "\n",
    "def cost_function(w, X, y, type_f='mse'):\n",
    "    \"\"\"\n",
    "    Returns a cost function of a linear model with coefficients w\n",
    "    oh features X and labels y.\n",
    "    \"\"\"\n",
    "    if (type_f == 'mae'):\n",
    "        return mean_absolute_error(y, linear_prediction(w, X))\n",
    "    elif (type_f == 'mse'):\n",
    "        return mean_squared_error(y, linear_prediction(w, X))\n",
    "    else:\n",
    "        #raise\n",
    "        print('error: incorrect type of function')\n",
    "        return\n",
    "\n",
    "def linear_regression_fit(X, y, minimize='grad_desc', cost_f='mse',\n",
    "                          w0=None, eta=1e-2):\n",
    "    \"\"\"\n",
    "    Returns weights that minimizes cost function.\n",
    "    \"\"\"   \n",
    "    if (minimize == 'analytical'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        # X_t = X.transpose()\n",
    "        # w = np.linalg.inv(X_t.dot(X)).dot(X_t).dot(y)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = np.linalg.lstsq(X,y)[0]\n",
    "        \n",
    "    elif (minimize == 'grad_desc'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = gradient_descent(X, y, w0, eta)[0]\n",
    "        w = w.reshape(w.shape[0])\n",
    "\n",
    "    elif (minimize == 'st_grad_desc'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = stochastic_gradient_descent(X, y, w0, eta)[0]\n",
    "        w = w.reshape(w.shape[0])\n",
    "    \n",
    "    elif (minimize == 'scipy_minimize'):\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        w = scipy.optimize.minimize(lambda w: cost_function(w, X, y, cost_f), w0).x\n",
    "    else:\n",
    "        #raise\n",
    "        print('error: incorrect minimization method')\n",
    "        return\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_step(X, y, w, eta=0.01):\n",
    "    grad = 2 * X.T.dot(X.dot(w) - y) / X.shape[0] \n",
    "    w_next = w - eta * grad \n",
    "    return w_next\n",
    "\n",
    "def gradient_descent(X, y, w0=None, eta=1e-2,\n",
    "                     max_iter=1e4, min_weight_dist=1e-8):    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if(w0 == None):\n",
    "        w0 = np.zeros(X.shape[1])\n",
    "        \n",
    "    w = w0\n",
    "    weight_dist = np.inf\n",
    "    errors = []\n",
    "    iter_num = 0\n",
    "        \n",
    "    while weight_dist > min_weight_dist and iter_num < max_iter: \n",
    "        w_next = gradient_step(X, y, w, eta)        \n",
    "        errors.append(mean_squared_error(y, X.dot(w_next)))\n",
    "        weight_dist = np.linalg.norm(w_next - w)      \n",
    "        w = w_next\n",
    "        iter_num += 1\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_step(X, y, w, k, eta=0.01):\n",
    "    grad = 2 * (X[k].dot(w) - y[k]) * X[k,:] / X.shape[0] \n",
    "    w_next = w - eta * grad   \n",
    "    return w_next\n",
    "\n",
    "def stochastic_gradient_descent(X, y, w0=None, eta=1e-2, max_iter=1e4,\n",
    "                                min_weight_dist=1e-8, seed=42):    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if(w0 == None):\n",
    "        w0 = np.zeros(X.shape[1])\n",
    "        \n",
    "    w = w0\n",
    "    weight_dist = np.inf\n",
    "    errors = []\n",
    "    iter_num = 0\n",
    "    #import random\n",
    "    #k = np.array(range(X.shape[0]))\n",
    "    #random.shuffle(k)\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    while weight_dist > min_weight_dist and iter_num < max_iter:  \n",
    "        #print('\\nit.',iter_num)\n",
    "        #print('w: ',w)\n",
    "        random_ind = np.random.randint(X.shape[0])\n",
    "        w_next = stochastic_gradient_step(X, y, w, random_ind, eta)     \n",
    "        errors.append(mean_squared_error(y, X.dot(w_next)))\n",
    "        weight_dist = np.linalg.norm(w_next - w)      \n",
    "        #print('d: ',weight_dist)\n",
    "        w = w_next\n",
    "        iter_num += 1\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(coef, true, predict, cut = 5):\n",
    "    print('w:\\n',coef,'\\n')\n",
    "    print('true vs. prediction:\\n',np.vstack((true,predict)).T[:cut],'\\n...')\n",
    "    print('root mean squared error: ',round((mean_squared_error(true, predict))**.5,3))\n",
    "    print('mean absolute error: ',round(mean_absolute_error(true, predict),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_for_one_feature(train_data, train_labels, w, title):\n",
    "    x = np.linspace(train_data.min(), train_data.max(), 2).reshape((2,1))\n",
    "    plt.figure(figsize = (8,5))\n",
    "    plt.plot(train_data, train_labels, 'o', markersize = 3)\n",
    "    #print(x)\n",
    "    #print(linear_prediction(w, x))\n",
    "    plt.plot(x, linear_prediction(w,x), '-', linewidth = 4)\n",
    "    plt.xlabel('feature')\n",
    "    plt.ylabel('label')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, mean_std=True):\n",
    "    if mean_std:\n",
    "        means, stds = X.mean(axis=0), X.std(axis=0)\n",
    "        X = (X - means) / stds\n",
    "    else:\n",
    "        minim, maxim = data.min(axis = 0), data.max(axis = 0)\n",
    "        X = (X - minim) / (maxim - minim)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  75.87807753,  -25.62849436,   -8.52830532,   34.7295849 ,\n",
       "         14.94573823,  -43.80916304,  -18.58567559,  -16.07885396,\n",
       "         12.29620556,   -2.59927619,  -37.8086868 ,  -12.76903779,\n",
       "        -27.47018765,  -27.61324039,  -17.26011438,   26.52165317,\n",
       "          1.73192444,   44.15478142,  -14.00371474,   39.03303189,\n",
       "         49.8749579 ,   34.47979933,   15.80407304,  -37.78326744,\n",
       "        -75.63811301,   17.94076279,  -58.97860917,   36.61443886,\n",
       "         36.61808062,   -2.8224484 ,   -0.69879232,   47.26971358,\n",
       "        104.46237473,   36.82761358,  -44.39979228,  -15.00089409,\n",
       "        -28.46821439,  -48.48269864,  -27.05864665,  -40.52520996,\n",
       "         -4.50734694,  -50.21672081,   -7.02962287,    7.4786154 ,\n",
       "          8.34655722,   -6.19297489,    5.31972292,   55.88383884,\n",
       "         16.43721584,   94.1177299 ,   42.37765206,  -38.65087801,\n",
       "         86.82359442,   18.55162032,  -28.31097045,   44.52145512,\n",
       "         -7.38026943,    9.49211339,   45.78374099,  -13.52407556,\n",
       "        -46.55029575,   -0.76347855,   38.2798966 ,   -4.86428637,\n",
       "         27.57070316,   -9.09063596,   19.64481823,   -2.10442878,\n",
       "         -0.48511024,  -81.56692398,  -63.10178965,  -21.8937501 ,\n",
       "        -17.48541077,   65.58634614,  -22.97550472,  -15.69256785,\n",
       "       -102.64545201,  -45.89170689,    6.61232416,  -48.68580244,\n",
       "         -1.8659642 ,  -57.70603736,  -33.38938267,  -94.22782206,\n",
       "        -11.81487   ,  -11.74147363,   41.42987704,   19.40645033,\n",
       "         81.9713515 ,  -41.46821271,  -53.67720435,  -59.83622739,\n",
       "         30.13166165,  -41.09935052,  -51.43366095,   -3.97053884,\n",
       "         36.08041471,   -1.02661171,  -66.74577182,   20.84887335,\n",
       "         16.24212205,   -4.07359572,    5.25235503,  -35.40924912,\n",
       "         16.82704337,   27.45436525,   13.92233204,   33.44596107,\n",
       "         -7.14777713,   25.3778504 ,    0.5082915 ,   -7.4477305 ,\n",
       "        -72.11005503,   67.76396189,  -95.82254917,  -27.12995714,\n",
       "         71.0049104 ,  -13.26392817,   13.21204327,  -45.81625982,\n",
       "        -45.3163235 ,   10.97979341,    2.06975075,   21.83248649,\n",
       "         30.97142932,   19.7583686 ,   26.42075197,   48.66985917,\n",
       "         17.01843985,   -7.66477104,  -38.83176442,   22.0642714 ,\n",
       "         60.62198504,   54.20147019,  -21.99624141,   -5.23392198,\n",
       "         -7.68840044,   -1.33413704,  -14.0059177 ,   30.27261642,\n",
       "        -52.39309738,   65.72963734,   28.95599495,   16.21313984,\n",
       "         39.5739653 ,   17.95017723,   52.00484206,  -12.32928934,\n",
       "        -21.56734831,  -72.61025127,   -7.11134869,  -25.36103814,\n",
       "        -40.97991472,   -1.50288825,   22.9573788 ,   43.59600288,\n",
       "         26.54114684,  -70.97179112,    3.29878656,  -26.76648806,\n",
       "        -26.68110795,   32.51976468,  -65.51163899,   23.20708812,\n",
       "         60.33885292,  -89.80419985,  -19.51724101,   13.50230803,\n",
       "        -30.49979347,   -7.82347422,  -52.78692326,    0.60956946,\n",
       "        125.73633586,   30.72219326,   13.64939566,   -4.6841355 ,\n",
       "         32.63963238,   31.14448044,   -2.79509552,   -7.26671637,\n",
       "         21.51506995,    6.01182641,   50.12399535,   43.71528504,\n",
       "        -17.59783545,   50.24020603,  156.08507321,    1.12163037,\n",
       "         54.85744837,   -5.11886347,  -28.45447281,   -5.25739862,\n",
       "         19.58060379,   61.48283003,  -56.62099641,  -64.45383678,\n",
       "         58.07305762,  -32.37817113,  -72.05431672,  -36.45553508])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "\n",
    "sample_size = 200\n",
    "data, target = datasets.make_regression(n_samples = sample_size,\n",
    "                                        n_features = 1, \n",
    "                                        n_informative = 1, \n",
    "                                        n_targets = 1, noise = 5.,\n",
    "                                        coef = False, random_state = 2)\n",
    "\n",
    "#data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \"Advertising\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('Data/weights_heights.csv', index_col='Index')\n",
    "data_frame = pd.read_csv('Data/advertising.csv')\n",
    "\n",
    "features = ['TV'] # , 'Radio','Newspaper']\n",
    "labels = ['Sales']\n",
    "data = np.array(data_frame[features].values, dtype=float)[:sample_size]\n",
    "target = np.array(data_frame[labels].values, dtype=float)[:sample_size]\n",
    "target = target.reshape(target.shape[0])\n",
    "\n",
    "data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset \"Boston\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_boston()\n",
    "data = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train & test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cross_val\n",
    "\n",
    "train_data, test_data, \\\n",
    "train_labels, test_labels = cross_val.train_test_split(data, target,\n",
    "                                                       test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: \n",
      " [[-0.40766012  0.97154295 -0.73637217 -0.27259857 -1.05124209  0.2996992\n",
      "  -1.78424818  0.80653853 -0.29308074 -0.47061187 -1.08911039  0.29533561\n",
      "  -0.55832104]\n",
      " [-0.40354288 -0.48772236 -0.37597609 -0.27259857 -0.29970737  0.26978136\n",
      "   1.01436883 -0.6475206  -0.52300145 -0.14395131  1.13022958  0.4228511\n",
      "  -0.05369542]\n",
      " [ 1.07222115 -0.48772236  1.01599907 -0.27259857  1.60072524 -0.61350702\n",
      "   0.99658854 -0.90293643  1.66124525  1.53092646  0.80657583 -1.27355443\n",
      "   1.56110656]\n",
      " [-0.20492927 -0.48772236  1.2319449   3.66839786  0.43455068  2.16172808\n",
      "   1.05348546 -0.83396037 -0.52300145 -0.03110494 -1.73641788  0.36112176\n",
      "  -1.50449408]\n",
      " [-0.41115675 -0.48772236  0.11573841 -0.27259857  0.15812412  0.43931575\n",
      "   0.01867281 -0.62579623 -0.98284286 -0.80321172  1.17646583  0.38721693\n",
      "  -0.41814726]] \n",
      "...\n",
      "\n",
      "train_labels: \n",
      " [ 26.4  19.8  10.8  50.   22.4] ...\n"
     ]
    }
   ],
   "source": [
    "print('train_data: \\n',train_data[:5],'\\n...\\n')\n",
    "print('train_labels: \\n',train_labels[:5],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical OLS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44376062  -0.49832497   1.1037074    0.36817348   1.12305293\n",
      "  -1.86223667   3.39902492  -0.30222417  -2.83374334   2.1979636\n",
      "  -2.03170579  -1.85956253   0.93330698  -3.16650413] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.82826868]\n",
      " [ 22.          26.08788447]\n",
      " [ 13.4         16.39740984]\n",
      " [ 30.8         31.26097619]\n",
      " [ 24.3         23.43286557]] \n",
      "...\n",
      "root mean squared error:  5.448\n",
      "mean absolute error:  3.546\n"
     ]
    }
   ],
   "source": [
    "w = linear_regression_fit(train_data, train_labels, minimize='analytical')\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44376062  -0.49832493   1.10370736   0.36817327   1.12305296\n",
      "  -1.86223661   3.39902497  -0.30222419  -2.83374335   2.19796316\n",
      "  -2.03170529  -1.8595625    0.93330697  -3.16650412] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.82826874]\n",
      " [ 22.          26.08788453]\n",
      " [ 13.4         16.39740995]\n",
      " [ 30.8         31.26097613]\n",
      " [ 24.3         23.43286543]] \n",
      "...\n",
      "root mean squared error:  5.448\n",
      "mean absolute error:  3.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels,\n",
    "                          minimize='grad_desc', eta=0.1)\n",
    "\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 21.05828054  -0.44114324   0.90649008  -0.39014135   1.02538009\n",
      "  -0.86013865   3.59852964   0.1829777   -1.66467708   0.34865592\n",
      "  -0.42275516  -1.71221661   0.66769586  -2.78834143] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.51560566]\n",
      " [ 22.          24.85790143]\n",
      " [ 13.4         16.76402607]\n",
      " [ 30.8         28.65614294]\n",
      " [ 24.3         21.26785382]] \n",
      "...\n",
      "root mean squared error:  5.874\n",
      "mean absolute error:  3.756\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels,\n",
    "                          minimize='st_grad_desc', eta=0.05)\n",
    "\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 21.57250514  -0.25650434   0.61159515   0.21744535   0.38726468\n",
      "  -0.81356805   3.97613022  -0.70018654  -1.83983897   1.08398396\n",
      "  -1.79501132  -1.61558548   1.0326189   -2.54325451] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.83540185]\n",
      " [ 22.          25.86114687]\n",
      " [ 13.4         14.83382662]\n",
      " [ 30.8         29.77006751]\n",
      " [ 24.3         22.13718297]] \n",
      "...\n",
      "root mean squared error:  5.706\n",
      "mean absolute error:  3.508\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels, minimize='scipy_minimize', cost_f='mae')\n",
    "\n",
    "w.reshape((1,w.shape[0]))\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.linear_model.SGDRegression for check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44025172  -0.32870933   0.91243119  -0.04248737   1.21194893\n",
      "  -1.47927849   3.54029591  -0.20553943  -2.55304485   1.26651369\n",
      "  -0.98148952  -1.73279411   0.91530178  -3.15202024] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         19.16797719]\n",
      " [ 22.          26.28505887]\n",
      " [ 13.4         17.20411417]\n",
      " [ 30.8         30.52080577]\n",
      " [ 24.3         22.95317484]] \n",
      "...\n",
      "root mean squared error:  5.527\n",
      "mean absolute error:  3.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model \n",
    "\n",
    "sgd_regressor = linear_model.SGDRegressor(random_state=None, n_iter=20)\n",
    "sgd_regressor.fit(train_data, train_labels)\n",
    "sgd_regressor.predict(test_data)\n",
    "w = [sgd_regressor.intercept_[0]]\n",
    "w.extend(sgd_regressor.coef_)\n",
    "w = np.array(w)\n",
    "print_result(w,test_labels,sgd_regressor.predict(test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TRASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(k, dim, low, high):\n",
    "    \"\"\"\n",
    "    initialize population of k elements of dimension = dim\n",
    "    low is the lowest value of possible element\n",
    "    high is the highest value of possible element\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    init_population = np.array([np.array((high[i] - low[i]) * \\\n",
    "                                         random.sample(dim) + low[i])\n",
    "                                for i in range(k)])\n",
    "    return init_population\n",
    "\n",
    "def evaluate(fit, population):\n",
    "    \"\"\"\n",
    "    evaluate fitness-function of each element from population\n",
    "    \"\"\"\n",
    "    return np.array([fit(element) for element in population])\n",
    "\n",
    "def select(population):\n",
    "    \"\"\"\n",
    "    randomly select two elements from population\n",
    "    \"\"\"\n",
    "    indices = np.array(range(population.shape[0]))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:2]\n",
    "    return population[indices]\n",
    "\n",
    "def genotype(phenotype):\n",
    "    \"\"\"\n",
    "    return genotype as a binary represantation of phenotype\n",
    "    \"\"\"\n",
    "    return int(bin(phenotype)[2:])\n",
    "\n",
    "def phenotype(genotype):\n",
    "    \"\"\"\n",
    "    return phenotype as a decimal represantation of genotype\n",
    "    \"\"\"\n",
    "    return int(str(genotype), 2)\n",
    "\n",
    "def crossover(selected, p=0.5):\n",
    "    n_coord = selected.shape[1]\n",
    "    new_element = np.zeros(n_coord)\n",
    "    for i in range(n_coord):\n",
    "        new_element[i] = np.random.choice(selected[:,i], p=[p, 1-p])\n",
    "        print(selected[:,i])\n",
    "    return new_element\n",
    "\n",
    "def mutation(selected, p=0.5):\n",
    "    return\n",
    "\n",
    "def inversion(selected, p=0.5):\n",
    "    return\n",
    "\n",
    "def stop_condition(f):\n",
    "    return\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
